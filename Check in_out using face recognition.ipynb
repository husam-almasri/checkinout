{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install tensorflow opencv-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard dependencies\n",
    "import cv2\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import metric calculations\n",
    "from tensorflow.keras.metrics import Precision, Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import uuid library to generate unique image names\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import tensorflow dependencies - Functional API\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Layer, Conv2D, Dense, MaxPooling2D, Input, Flatten\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus: \n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Create Folder Structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create paths to directories for negative, positive, and anchor samples.\n",
    "\n",
    "The code block uses the `os.path.join()` function to create paths to directories \n",
    "for negative, positive, and anchor samples in a machine learning or computer vision project. \n",
    "\n",
    "Args:\n",
    "-----\n",
    "NEG_PATH : str\n",
    "    The path to the directory containing negative samples.\n",
    "\n",
    "POS_PATH : dict\n",
    "    A dictionary containing paths to directories for positive samples, with keys \n",
    "    of the form \"POS{i}_PATH\", where `{i}` is an integer between 1 and 5.\n",
    "\n",
    "ANC_PATH : dict\n",
    "    A dictionary containing paths to directories for anchor samples, with keys \n",
    "    of the form \"ANC{i}_PATH\", where `{i}` is an integer between 1 and 5.\n",
    "\n",
    "The values for the keys in `POS_PATH` and `ANC_PATH` are created using the `os.path.join()` \n",
    "function to join the path components `'data'`, `'positive{i}'` or `'anchor{i}'`, where `{i}` \n",
    "is an integer between 1 and 5.\n",
    "\n",
    "Returns:\n",
    "-------\n",
    "None\n",
    "\n",
    "\"\"\" \n",
    "\n",
    "NEG_PATH = os.path.join('data', 'negative')\n",
    "\n",
    "POS_PATH={}\n",
    "ANC_PATH={}\n",
    "for i in range(1, 6):\n",
    "    POS_PATH[f\"POS{i}_PATH\"] = os.path.join('data', f'positive{i}')\n",
    "    ANC_PATH[f\"ANC{i}_PATH\"] = os.path.join('data', f'anchor{i}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create directories for negative, positive, and anchor samples.\n",
    "\n",
    "The code block creates directories for negative, positive, and anchor samples in a machine \n",
    "learning or computer vision project using the `os.makedirs()` function. \n",
    "\n",
    "Args:\n",
    "-----\n",
    "NEG_PATH : str\n",
    "    The path to the directory containing negative samples.\n",
    "\n",
    "POS_PATH : dict\n",
    "    A dictionary containing paths to directories for positive samples, with keys \n",
    "    of the form \"POS{i}_PATH\", where `{i}` is an integer between 1 and 5.\n",
    "\n",
    "ANC_PATH : dict\n",
    "    A dictionary containing paths to directories for anchor samples, with keys \n",
    "    of the form \"ANC{i}_PATH\", where `{i}` is an integer between 1 and 5.\n",
    "\n",
    "`os.makedirs()` is used to create the directories specified by `NEG_PATH`, `POS_PATH`, \n",
    "and `ANC_PATH`.\n",
    "\n",
    "Returns:\n",
    "-------\n",
    "None\n",
    "\n",
    "\"\"\"\n",
    "os.makedirs(NEG_PATH)\n",
    "paths=[POS_PATH,ANC_PATH]\n",
    "for p in paths:\n",
    "    for value in p.values():\n",
    "        os.makedirs(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Collect Negatives, Positives and Anchors images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to create a diverse and comprehensive dataset for training and testing the model, I am using three different sources. \n",
    "\n",
    "1- The first source is the \"Labeled Faces in the Wild\" (LFW) dataset, which contains thousands of images of celebrities in various poses and lighting conditions. \n",
    "\n",
    "2- The DrivFace dataset, which contains images of drivers taken from inside a car, with varying expressions and lighting conditions. \n",
    "\n",
    "3-I am taking a set of images using my laptop's camera and the OpenCV library. \n",
    "This allows me to capture images of myself under various lighting conditions and facial expressions, and add them to the dataset for increased diversity.\n",
    "\n",
    "By combining these three sources, I am creating a dataset that is robust and diverse enough to effectively train and test the model on a wide range of real-world scenarios.\n",
    "\n",
    "To ensure that the dataset is balanced and contains both positive and negative examples, I will use the LFW dataset as the source for negative images, while the other sources will be used to create positive and anchor images. \n",
    "\n",
    "Positive images will be those that contain a face that matches the anchor image, while anchor images are those that are randomly selected from the dataset and used as a reference for comparison. \n",
    "\n",
    "By using the LFW dataset as the source for negative images, I can ensure that the model is trained to accurately distinguish between faces that are present in the dataset and those that are not, which is crucial for effective face recognition in real-world scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Negative Images: Untar LFW Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The link http://vis-www.cs.umass.edu/lfw/ is the homepage of the \"Labeled Faces in the Wild\" (LFW) dataset, which is a popular benchmark dataset in computer vision. The dataset contains over 13,000 images of faces collected from the internet, and is widely used for tasks such as face recognition and verification. The LFW dataset is notable for its large size, diversity of subjects, and real-world conditions, such as variability in pose, lighting, and facial expressions. The dataset is freely available for download, and has been used in numerous research papers and competitions in computer vision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Extract files from a compressed tar archive.\n",
    "\n",
    "The code block uses the `tar` command to extract files from a compressed tar \n",
    "archive file named `lfw.tgz`. The `-xf` options are used with the `tar` command\n",
    "to extract the contents of the archive and preserve file permissions and\n",
    "ownership.\n",
    "\n",
    "\"\"\" \n",
    "# !tar -xf lfw.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Copy files from the LFW dataset to the negative sample directory.\n",
    "\n",
    "The code block loops over each directory and file in the `lfw` directory, which contains \n",
    "the Labeled Faces in the Wild (LFW) dataset. It uses the `shutil.copy()` function to copy \n",
    "each file to the `NEG_PATH` directory, which is a directory for storing negative samples.\n",
    "\n",
    "Args:\n",
    "-----\n",
    "NEG_PATH : str\n",
    "    The path to the directory containing negative samples.\n",
    "\n",
    "Returns:\n",
    "-------\n",
    "None\n",
    "\n",
    "\"\"\"\n",
    "for directory in os.listdir('lfw'):\n",
    "    for file in os.listdir(os.path.join('lfw', directory)):\n",
    "        EX_PATH = os.path.join('lfw', directory, file)\n",
    "        NEW_PATH = os.path.join(NEG_PATH, file)\n",
    "        shutil.copy(EX_PATH, NEW_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Collect Positive and Anchor Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Crop images in a folder to a specified size.\n",
    "\n",
    "The `crop_img()` function takes a folder path, height shift, and width shift as input \n",
    "arguments, and crops each image in the folder to a specified size. The function loops \n",
    "through each file in the folder, opens the image, calculates the crop box coordinates \n",
    "based on the current image size and the specified size, and crops the image. The cropped \n",
    "image is then saved with the original filename.\n",
    "\n",
    "Args:\n",
    "-----\n",
    "folder_path : str\n",
    "    The path to the folder containing the images to be cropped.\n",
    "height_shift : int\n",
    "    The amount to shift the height of the crop box.\n",
    "width_shift : int\n",
    "    The amount to shift the width of the crop box.\n",
    "\n",
    "Returns:\n",
    "-------\n",
    "None\n",
    "\n",
    "\"\"\"\n",
    "def crop_img(folder_path,height_shift, width_shift):\n",
    "    # set the size of the cropped image\n",
    "    crop_size = (250, 250)\n",
    "\n",
    "    # loop through all files in the folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        # open the image file\n",
    "        image_path = os.path.join(folder_path, filename)\n",
    "        img = cv2.imread(image_path)\n",
    "        # get the current size of the image\n",
    "        height, width, _ = img.shape\n",
    "        # calculate the coordinates of the top-left corner of the crop box\n",
    "        left = int(((width+width_shift) - crop_size[0]) / 2)\n",
    "        top = int(((height+height_shift) - crop_size[1]) / 2)\n",
    "        right = int(((width+width_shift) + crop_size[0]) / 2)\n",
    "        bottom = int(((height+height_shift) + crop_size[1]) / 2)\n",
    "        # crop the image\n",
    "        cropped_img = img[top:bottom, left:right]\n",
    "        # save the cropped image with the original filename\n",
    "        cv2.imwrite(os.path.join(folder_path, filename), cropped_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step, we should performs the following operations:\n",
    "\n",
    "1- Iterates through files in the DrivImages* directory that contain a substring '0{number}' in their name.\n",
    "\n",
    "2- Creates a new subdirectory in the data directory with the name anchor{number} if it does not exist.\n",
    "\n",
    "3- Copies the selected files from DrivImages to the new subdirectory.\n",
    "\n",
    "4- Calls the crop_img() function to crop the images in the new subdirectory to a size of 250x250.\n",
    "\n",
    "5- Selects the first half of the files in the new subdirectory.\n",
    "\n",
    "6- Moves the selected files to a new directory with the name positive{number} in the data directory.\n",
    "\n",
    "*/ The DrivFace dataset is a collection of images designed for use in computer vision and machine learning applications. \n",
    "\n",
    "The dataset was collected using a camera mounted inside a car and includes images of drivers with varying head poses and lighting conditions. The dataset contains approximately 606 images of 4 individuals, with each individual's face captured under a range of different driving conditions. \n",
    "\n",
    "Along with the images, the dataset also includes metadata such as the driver's age, gender, and driving experience. \n",
    "\n",
    "The DrivFace dataset has been widely used in research on facial recognition, head pose estimation, and other computer vision tasks, and is available for download from the UCI Machine Learning Repository https://archive.ics.uci.edu/ml/datasets/DrivFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Args:\n",
    "-----\n",
    "folder_path : str\n",
    "    A string representing the path to the directory where the data\n",
    "    will be stored.\n",
    "\n",
    "src_path : str\n",
    "    A string representing the path to the directory containing the source\n",
    "    images.\n",
    "\n",
    "number : int\n",
    "    An integer representing the number of the anchor image being processed.\n",
    "\n",
    "Returns:\n",
    "-------\n",
    "\n",
    "None. The function performs file I/O and image processing operations on the\n",
    "specified directories.\n",
    "\n",
    "\"\"\"\n",
    "# set the path of the folder containing the files\n",
    "folder_path = 'data'\n",
    "src_path = 'DrivImages'\n",
    "for number in range (1,5):\n",
    "    subfolder_path = os.path.join(folder_path, f'anchor{number}')\n",
    "    # loop through all files in the folder\n",
    "    for filename in os.listdir(src_path):\n",
    "        # check if the file contains the substring '_Number_' in its name\n",
    "        if f'_0{number}_' in filename:\n",
    "            \n",
    "            # create a new folder with the subfolder name if it doesn't already exist\n",
    "            if not os.path.exists(subfolder_path):\n",
    "                os.mkdir(subfolder_path)\n",
    "            # copy the file to the new subfolder\n",
    "            src_file_path = os.path.join(src_path, filename)\n",
    "            dst_file_path = os.path.join(subfolder_path, filename)\n",
    "            shutil.copy(src_file_path, dst_file_path)\n",
    "# Crop images to 250x250\n",
    "    if number==1:\n",
    "        crop_img(subfolder_path,10, 50)\n",
    "    elif number==3:\n",
    "        crop_img(subfolder_path,150, 50)\n",
    "    elif number==2 or number==4:\n",
    "        crop_img(subfolder_path,-50, 50)\n",
    "#Move half of images to positive folder\n",
    "    # Calculate the index to split the files list in half\n",
    "    half_index = len(os.listdir(subfolder_path))//2\n",
    "\n",
    "    # Select the first half of the files to move\n",
    "    files_to_move = os.listdir(subfolder_path)[:half_index]\n",
    "\n",
    "    # Loop over the files to move and move each one to the destination folder\n",
    "    for file_name in files_to_move:\n",
    "        source_path = os.path.join(subfolder_path, file_name)\n",
    "        destination_path = os.path.join(folder_path, f'positive{number}', file_name)\n",
    "        shutil.move(source_path, destination_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to creat the third package of images that is used in the training, we establishes a connection to the webcam and captures frames. \n",
    "\n",
    "It crops the frame to a size of 250x250 pixels and waits for the user to press the 'a' key to save the current frame as an anchor image or the 'p' key to save it as a positive image. \n",
    "\n",
    "It displays the frame to the screen and breaks the loop when the user presses the 'q' key. \n",
    "\n",
    "Finally, it releases the webcam and closes the window displaying the captured images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Establish a connection to the webcam and capture frames. The captured frame is \n",
    "cropped to a size of 250x250 pixels. The user can press the 'a' key to save the current \n",
    "frame as an anchor image or the 'p' key to save it as a positive image. The captured \n",
    "image is displayed to the screen. The loop ends when the user presses the 'q' key. \n",
    "The webcam is then released and the window displaying the captured images is closed.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "while cap.isOpened(): \n",
    "    ret, frame = cap.read()\n",
    "   \n",
    "    # Cut down frame to 250x250px\n",
    "    frame = frame[120:120+250,200:200+250, :]\n",
    "    \n",
    "    # Collect anchors \n",
    "    if cv2.waitKey(1) & 0XFF == ord('a'):\n",
    "        # Create the unique file path \n",
    "        imgname = os.path.join(ANC_PATH['ANC5_PATH'], '{}.jpg'.format(uuid.uuid1()))\n",
    "        # Write out anchor image\n",
    "        cv2.imwrite(imgname, frame)\n",
    "    \n",
    "    # Collect positives\n",
    "    if cv2.waitKey(1) & 0XFF == ord('p'):\n",
    "        # Create the unique file path \n",
    "        imgname = os.path.join(POS_PATH['POS5_PATH'], '{}.jpg'.format(uuid.uuid1()))\n",
    "        # Write out positive image\n",
    "        cv2.imwrite(imgname, frame)\n",
    "    \n",
    "    # Show image back to screen\n",
    "    cv2.imshow('Image Collection', frame)\n",
    "    \n",
    "    # Breaking gracefully\n",
    "    if cv2.waitKey(1) & 0XFF == ord('q'):\n",
    "        break\n",
    "        \n",
    "# Release the webcam\n",
    "cap.release()\n",
    "# Close the image show frame\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform data augmentation on an input image by taking a single input parameter \"img\", which is the image to be augmented. \n",
    "\n",
    "Data augmentation can be useful for generating a larger and more diverse dataset for training deep learning models.\n",
    "\n",
    "The data augmentation process involves applying a set of random transformations to the input image. Specifically, the function applies:\n",
    "\n",
    "1- A stateless random brightness adjustment.\n",
    "\n",
    "2- A stateless random contrast adjustment.\n",
    "\n",
    "3- A stateless random flip left-right.\n",
    "\n",
    "4- A stateless random JPEG quality adjustment.\n",
    "\n",
    "5- A stateless random saturation adjustment. \n",
    "\n",
    "The function iterates over the image nine times, applying a new set of random transformations to the image at each iteration. \n",
    "\n",
    "The resulting augmented images are stored in a list called \"data\" and are returned by the function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "The data_aug function applies several data augmentation techniques to an input \n",
    "image and returns a list of the augmented images.\n",
    "\n",
    "Args:\n",
    "-----\n",
    "img: \n",
    "    a tensor representing an image, with shape (height, width, channels).\n",
    "\n",
    "Returns:\n",
    "-------\n",
    "data: list\n",
    "    a list of 9 tensors representing the augmented images. \n",
    "    Each tensor has the same shape as the input image. The augmentation techniques \n",
    "    applied include random brightness adjustment, random contrast adjustment,\n",
    "    random horizontal flipping, random JPEG quality reduction, and random \n",
    "    saturation adjustment.\n",
    "    The random seed used for each augmentation is determined by a combination of\n",
    "    fixed and randomly generated seeds.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def data_aug(img):\n",
    "    \n",
    "    data = []  # initialize an empty list to store augmented images\n",
    "    \n",
    "    for i in range(9):  # iterate 9 times for data augmentation\n",
    "        \n",
    "        # Apply a stateless random brightness adjustment to the image\n",
    "        img = tf.image.stateless_random_brightness(img, max_delta=0.02, seed=(1,2))\n",
    "        \n",
    "        # Apply a stateless random contrast adjustment to the image\n",
    "        img = tf.image.stateless_random_contrast(img, lower=0.6, upper=1, seed=(1,3))\n",
    "        \n",
    "        # Apply a stateless random flip left-right to the image\n",
    "        img = tf.image.stateless_random_flip_left_right(img, seed=(np.random.randint(100),np.random.randint(100)))\n",
    "        \n",
    "        # Apply a stateless random JPEG quality adjustment to the image\n",
    "        img = tf.image.stateless_random_jpeg_quality(img, min_jpeg_quality=90, max_jpeg_quality=100, seed=(np.random.randint(100),np.random.randint(100)))\n",
    "        \n",
    "        # Apply a stateless random saturation adjustment to the image\n",
    "        img = tf.image.stateless_random_saturation(img, lower=0.9,upper=1, seed=(np.random.randint(100),np.random.randint(100)))\n",
    "        \n",
    "        # Append the augmented image to the data list\n",
    "        data.append(img)\n",
    "    \n",
    "    return data  # return the list of augmented images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "This code applies data augmentation to a set of positive and anchor images using\n",
    "the TensorFlow library. The augmented images are saved to the same directory as\n",
    "the original images with a unique file name generated using uuid.\n",
    "\n",
    "\"\"\"\n",
    "# Define a list of paths for positive and anchor images\n",
    "paths=[POS_PATH,ANC_PATH]\n",
    "\n",
    "# Loop through each path in the list of paths\n",
    "for p in paths:\n",
    "    \n",
    "    # Loop through each value (directory path) in the current path\n",
    "    for value in p.values():\n",
    "        \n",
    "        # Loop through each file name in the directory\n",
    "        for file_name in os.listdir(os.path.join(value)):\n",
    "            \n",
    "            # Get the image path for the current file name\n",
    "            img_path = os.path.join(value, file_name)\n",
    "            \n",
    "            # Read the image using OpenCV\n",
    "            img = cv2.imread(img_path)\n",
    "            \n",
    "            # Apply data augmentation to the image using the data_aug function defined earlier\n",
    "            augmented_images = data_aug(img) \n",
    "            \n",
    "            # Save each augmented image to the same directory with a unique file name using uuid\n",
    "            for image in augmented_images:\n",
    "                cv2.imwrite(os.path.join(value, '{}.jpg'.format(uuid.uuid1())), image.numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Load and Preprocess Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Get Image Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "The code creates TensorFlow datasets of image paths for negative, anchor,\n",
    "and positive images, each containing 450 images. \n",
    "The anchor and positive datasets are created for five different paths, \n",
    "identified by the loop variable i, with the paths determined by the variables \n",
    "ANC_PATH and POS_PATH that contain the path names for each anchor and positive\n",
    "dataset. \n",
    "Each dataset is stored in a separate global variable, with the variable name\n",
    "constructed using the string concatenation of \"anchor\" or \"positive\" with the\n",
    "loop variable i.\n",
    "\n",
    "Args:\n",
    "-----\n",
    "    NEG_PATH (str): \n",
    "        The path to the directory containing negative images.\n",
    "    ANC_PATH (dict): \n",
    "        A dictionary containing the paths to directories containing anchor images.\n",
    "    POS_PATH (dict): \n",
    "        A dictionary containing the paths to directories containing positive images.\n",
    "    \n",
    "Returns:\n",
    "-------\n",
    "    None\n",
    "\n",
    "\"\"\"\n",
    "# Create a TensorFlow dataset of negative image paths and take the first 450\n",
    "negative = tf.data.Dataset.list_files(NEG_PATH+'\\*.jpg').take(450)\n",
    "\n",
    "# Loop from 1 to 5 (inclusive)\n",
    "for i in range(1,6):\n",
    "\n",
    "    # Create a TensorFlow dataset of anchor image paths and take the first 450, and store in a global variable\n",
    "    globals()[\"anchor\"+str(i)]=tf.data.Dataset.list_files(ANC_PATH[f'ANC{i}_PATH']+'\\*.jpg').take(450)\n",
    "    \n",
    "    # Create a TensorFlow dataset of positive image paths and take the first 450, and store in a global variable\n",
    "    globals()[\"positive\"+str(i)]=tf.data.Dataset.list_files(POS_PATH[f'POS{i}_PATH']+'\\*.jpg').take(450)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Print each element in the `negative` dataset.\n",
    "It is important to note that if negative was created using the list_files()\n",
    "method, the order of the elements may be shuffled each time the dataset is \n",
    "called due to the shuffling behavior of list_files().\n",
    "\n",
    "Args:\n",
    "-----\n",
    "    negative: \n",
    "        A TensorFlow dataset object containing a list of file paths to negative images.\n",
    "\n",
    "Returns:\n",
    "-------\n",
    "    None\n",
    "\n",
    "\"\"\"\n",
    "for i in negative:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Image Preprocessing - Scale and Resize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image preprocessing is an essential step in many computer vision applications, including object detection and recognition. \n",
    "\n",
    "One of the critical preprocessing steps is **Scaling and Resizing** the input images to a fixed size suitable for the model's input.\n",
    "\n",
    "Resizing and scaling images help in reducing the complexity of the model and improving its performance. Typically, image scaling and resizing are performed using a library like OpenCV or TensorFlow, which provide efficient and straightforward functions for these operations. \n",
    "\n",
    "Scaling and resizing images can also help in reducing the amount of memory required for storing the images, making it easier to process large datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    " \"\"\"\n",
    " \n",
    "Preprocesses an image file given its path, including reading it in, decoding it,\n",
    "resizing it to a fixed size, and scaling it to be between 0 and 1.\n",
    "\n",
    "Args:\n",
    "-----\n",
    " file_path: str\n",
    "        A string representing the file path of the image.\n",
    "\n",
    "Returns:\n",
    "-------\n",
    "    A tensor representing the preprocessed image with dimensions (105, 105, 3).\n",
    "\n",
    "\"\"\"\n",
    "def preprocess(file_path):\n",
    "    \n",
    "    # Read in image from file path\n",
    "    byte_img = tf.io.read_file(file_path)\n",
    "    \n",
    "    # Load in the image \n",
    "    img = tf.io.decode_jpeg(byte_img)\n",
    "    \n",
    "    # Preprocessing steps - resizing the image to be 105x105x3\n",
    "    img = tf.image.resize(img, (105,105))\n",
    "    \n",
    "    # Scale image to be between 0 and 1 \n",
    "    img = img / 255.0\n",
    "\n",
    "    # Return image\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Create Labelled Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once these images have been preprocessed and augmented, the next step is to combine them into a single dataset that can be used for training and testing. \n",
    "\n",
    "This is done by zipping the anchor, positive, and negative examples together, and then concatenating them into a single dataset. By doing this, we can ensure that the model is trained on a balanced set of anchor-positive-negative triplets, which are necessary for learning effective face recognition. \n",
    "\n",
    "Once the dataset is constructed, it can be used to train and test the model, and further evaluated using various performance metrics such as accuracy, precision, and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Construct a dataset by zipping anchor, positive, and negative examples and\n",
    "concatenating them. \n",
    "\n",
    "Args:\n",
    "-----\n",
    "    positives: A list of positive samples.\n",
    "    anchors: A list of anchor samples.\n",
    "    negative: A negative sample to be paired with anchor samples.\n",
    "\n",
    "Returns:\n",
    "-------\n",
    "    data: tf.data.Dataset\n",
    "        A dataset constructed by zipping anchor, positive, and negative examples and concatenating them.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Define datasets for positive and anchor data\n",
    "positives = [globals()[\"positive{}\".format(i)] for i in range(1, 6)]\n",
    "anchors = [globals()[\"anchor{}\".format(i)] for i in range(1, 6)]\n",
    "\n",
    "# Create datasets for negative and positive pairs\n",
    "datasets = []\n",
    "for i in range(5):\n",
    "    neg_dataset = tf.data.Dataset.zip((anchors[i], negative, tf.data.Dataset.from_tensor_slices(tf.zeros(len(anchors[i])))))\n",
    "    pos_dataset = tf.data.Dataset.zip((anchors[i], positives[i], tf.data.Dataset.from_tensor_slices(tf.ones(len(anchors[i])))))\n",
    "    datasets.extend([neg_dataset, pos_dataset])\n",
    "\n",
    "# Concatenate all the datasets\n",
    "data = datasets[0]\n",
    "for i in range(1, len(datasets)):\n",
    "    data = data.concatenate(datasets[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'data\\\\anchor1\\\\f1b67e89-de4b-11ed-b422-14abc5de9ec7.jpg'>, <tf.Tensor: shape=(), dtype=string, numpy=b'data\\\\negative\\\\Kim_Hong-up_0001.jpg'>, <tf.Tensor: shape=(), dtype=float32, numpy=0.0>)\n"
     ]
    }
   ],
   "source": [
    "# Print the first element in the concatenated dataset\n",
    "print(next(iter(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.data.ops.concatenate_op._ConcatenateDataset"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the concatenated dataset: 4500 rows\n"
     ]
    }
   ],
   "source": [
    "# Length of the concatenated dataset\n",
    "print(f'Length of the concatenated dataset: {len(data)} rows')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Build Train and Test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Preprocesses a pair of input and validation images, and their corresponding\n",
    "label.\n",
    "    \n",
    "Args:\n",
    "-----\n",
    "    - input_img (tf.Tensor): A tensor representing the input image.\n",
    "    - validation_img (tf.Tensor): A tensor representing the validation image.\n",
    "    - label (tf.Tensor): A tensor representing the label of the input image.\n",
    "\n",
    "Returns:\n",
    "-------\n",
    "    - A tuple containing three preprocessed tensors:\n",
    "        - The preprocessed input image tensor.\n",
    "        - The preprocessed validation image tensor.\n",
    "        - The original label tensor.\n",
    "        \n",
    "\"\"\"\n",
    "# Define a function called 'preprocess_twin' that takes in three parameters.\n",
    "def preprocess_twin(input_img, validation_img, label):\n",
    "    \n",
    "    # Return a tuple of three elements: \n",
    "    # 1- the preprocessed version of the input image using a function called 'preprocess',\n",
    "    # 2- the preprocessed version of the validation image using the same function, and\n",
    "    # 3- the label itself without any processing.\n",
    "    return(preprocess(input_img), preprocess(validation_img), label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Preprocesses the input data using the `preprocess_twin` function, then caches \n",
    "the preprocessed data, and finally shuffles it.\n",
    "\n",
    "Args:\n",
    "-----\n",
    "    data: A dataset containing the input data to be preprocessed, cached, and shuffled.\n",
    "\n",
    "Returns:\n",
    "-------\n",
    "    The preprocessed, cached, and shuffled dataset.\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "# Apply the function called 'preprocess_twin' to the input data using the 'map' function, which maps each element of the dataset through a function.\n",
    "data = data.map(preprocess_twin)\n",
    "\n",
    "# Cache the preprocessed dataset in memory to speed up access time for subsequent operations.\n",
    "data = data.cache()\n",
    "\n",
    "# Shuffle the preprocessed dataset using a buffer of size 10000 to ensure that the samples are randomly ordered.\n",
    "data = data.shuffle(buffer_size=10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Split the input data into training and testing datasets using the train_test_split function from the sklearn.model_selection module. \n",
    "\n",
    "args:\n",
    "-----\n",
    "    data (numpy array or pandas dataframe): \n",
    "        Input data to be split into training and testing datasets.\n",
    "        \n",
    "    test_size (float or int, default=0.3): \n",
    "        The proportion of the dataset to include in the test split.\n",
    "\n",
    "Returns:\n",
    "--------\n",
    "    tuple: \n",
    "        A tuple containing the training and testing datasets.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Split the data into training and testing sets with 70-30 ratio\n",
    "train_data = data.take(round(len(data)*.7))\n",
    "test_data = data.skip(round(len(data)*.7))\n",
    "test_data = test_data.take(round(len(data)*.3))\n",
    "\n",
    "\"\"\"\n",
    "Group the samples in the training and testing datasets into batches of size 16\n",
    "for more efficient processing.\n",
    "\n",
    "Pre-fetch the training and testing datasets to load the next batch of samples\n",
    "while the current batch is being processed, which can further speed up training.\n",
    "\"\"\"\n",
    "train_data = train_data.batch(16).prefetch(8)\n",
    "test_data = test_data.batch(16).prefetch(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data.save(\"train_data\")\n",
    "# test_data.save(\"test_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaded_train_data = tf.data.Dataset.load(\"train_data\")\n",
    "# loaded_test_data = tf.data.Dataset.load(\"test_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Model Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paper \"Siamese Neural Networks for One-shot Image Recognition\" by Koch et al. provided the inspiration for building a model for one-shot image recognition. \n",
    "\n",
    "The paper proposes a method for learning a similarity metric between two images using a Siamese neural network architecture. \n",
    "\n",
    "The Siamese network consists of two identical sub-networks that share the same set of weights, and the input images are passed through these sub-networks to produce feature vectors. The distance between the feature vectors of two images is then used as a measure of similarity between those images. \n",
    "\n",
    "This approach allows the model to learn to recognize objects even with very few training examples. The paper's experimental results showed that their method outperformed previous methods for one-shot image recognition, demonstrating the potential of Siamese networks in this domain. \n",
    "\n",
    "By adapting the Siamese network architecture and training process described in this paper to my specific use case, I was able to build a model that achieved impressive accuracy in recognizing similar faces in images with only a few examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Build Embedding Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function make_embedding() defines a sequential model to generate the embeddings for the Siamese network. \n",
    "\n",
    "The model has four convolutional layers with max pooling in between each layer. \n",
    "\n",
    "The first convolutional layer has 64 filters with a kernel size of (10,10) and an activation function of ReLU.\n",
    "\n",
    "The second and third convolutional layers have 128 filters each, with kernel sizes of (7,7) and (4,4), respectively, and both have ReLU activation functions.\n",
    "\n",
    "The final convolutional layer has 256 filters with a kernel size of (4,4) and also uses ReLU activation.\n",
    "\n",
    "The output of this layer is flattened and passed through a dense layer with 4096 units and a sigmoid activation function.\n",
    "\n",
    "This function returns the embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_embedding(): \n",
    "    \n",
    "    # define the sequential model\n",
    "    return Sequential([\n",
    "        \n",
    "        # First block\n",
    "        Conv2D(64, (10,10), activation='relu', input_shape=(105,105,3)),\n",
    "        MaxPooling2D(64, (2,2), padding='same'),\n",
    "        \n",
    "        # Second block\n",
    "        Conv2D(128, (7,7), activation='relu'),\n",
    "        MaxPooling2D(64, (2,2), padding='same'),\n",
    "\n",
    "        # Third block\n",
    "        Conv2D(128, (4,4), activation='relu'),\n",
    "        MaxPooling2D(64, (2,2), padding='same'),\n",
    "        \n",
    "        # Final embedding block\n",
    "        Conv2D(256, (4,4), activation='relu'),\n",
    "        Flatten(),\n",
    "        Dense(4096, activation='sigmoid')\n",
    "    ], name='embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "A function to create a sequential neural network model that consists of several\n",
    "convolutional layers and a dense layer with a sigmoid activation function.\n",
    "The resulting model is then assigned to the variable embedding. \n",
    "This model will be used for facial recognition in the subsequent steps of the \n",
    "code.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "embedding = make_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"embedding\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 96, 96, 64)        19264     \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 48, 48, 64)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 42, 42, 128)       401536    \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 21, 21, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 18, 18, 128)       262272    \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 9, 9, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 6, 6, 256)         524544    \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 9216)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4096)              37752832  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 38,960,448\n",
      "Trainable params: 38,960,448\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "This will print a summary of the embedding model's architecture, including the \n",
    "shape of the input and output tensors of each layer, the number of trainable\n",
    "parameters, and the activation functions used.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "embedding.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Build Distance Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L1 distance, also known as Manhattan distance, is a measure of distance between two points in a space. \n",
    "\n",
    "In the context of embeddings, the L1 distance between two embeddings is calculated by taking the absolute difference between the values in each corresponding position of the two embeddings, and summing them up. \n",
    "\n",
    "It is a common distance metric used in image recognition and other related fields, where it is used to compare the similarity between two images represented as embeddings.\n",
    "\n",
    "The smaller the L1 distance between two embeddings, the more similar the corresponding images are considered to be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class L1Dist(Layer):\n",
    "    \"\"\"\n",
    "    Layer that calculates the L1 distance between two embeddings.\n",
    "\n",
    "    Args:\n",
    "    -----\n",
    "        **kwargs: additional arguments to pass to parent class.\n",
    "\n",
    "    Methods:\n",
    "        call(self, input_embedding, validation_embedding): Returns the L1 distance between\n",
    "            input_embedding and validation_embedding using TensorFlow's abs() function.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def call(self, input_embedding, validation_embedding):\n",
    "        \"\"\"\n",
    "        Compute the L1 distance between two embeddings.\n",
    "\n",
    "        Args:\n",
    "        -----\n",
    "            input_embedding (tf.Tensor): the input embedding tensor.\n",
    "            validation_embedding (tf.Tensor): the validation embedding tensor.\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "            The L1 distance between the two input tensors.\n",
    "\n",
    "        \"\"\"\n",
    "        return tf.math.abs(input_embedding - validation_embedding)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Make Siamese Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function that creates a Siamese Network model that takes in two images (anchor and validation) and returns the probability that they belong to the same class. \n",
    "\n",
    "It uses the make_embedding() function to create an embedding layer that is used to calculate the L1 distance between the two input images.\n",
    "\n",
    "The L1 distance is then passed through a classification layer to predict whether the two images belong to the same class. \n",
    "\n",
    "The function returns the Siamese Network model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Creates a Siamese Network model that takes in two images (anchor and validation) \n",
    "and returns the probability that they belong to the same class. \n",
    "\n",
    "Returns:\n",
    "-------\n",
    "     Model: Siamese Network model\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def make_siamese_model(): \n",
    "    \n",
    "    # Anchor image input in the network\n",
    "    input_image = Input(name='input_img', shape=(105,105,3))\n",
    "    \n",
    "    # Validation image in the network \n",
    "    validation_image = Input(name='validation_img', shape=(105,105,3))\n",
    "    \n",
    "    # Combine siamese distance components\n",
    "    siamese_layer = L1Dist()\n",
    "    siamese_layer._name = 'distance'\n",
    "    distances = siamese_layer(embedding(input_image), embedding(validation_image))\n",
    "    \n",
    "    # Classification layer \n",
    "    classifier = Dense(1, activation='sigmoid')(distances)\n",
    "    \n",
    "    return Model(inputs=[input_image, validation_image], outputs=classifier, name='SiameseNetwork')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"SiameseNetwork\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_img (InputLayer)         [(None, 105, 105, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " validation_img (InputLayer)    [(None, 105, 105, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " embedding (Sequential)         (None, 4096)         38960448    ['input_img[0][0]',              \n",
      "                                                                  'validation_img[0][0]']         \n",
      "                                                                                                  \n",
      " distance (L1Dist)              (None, 4096)         0           ['embedding[0][0]',              \n",
      "                                                                  'embedding[1][0]']              \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 1)            4097        ['distance[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 38,964,545\n",
      "Trainable params: 38,964,545\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Creates a siamese network model using make_siamese_model() function.\n",
    "siamese_model = make_siamese_model()\n",
    "\n",
    "# Prints the model summary.\n",
    "siamese_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Setup Loss and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Create an instance of the BinaryCrossentropy class from the TensorFlow losses\n",
    "module and assigns it to the variable binary_cross_loss.\n",
    "\n",
    "This loss function is typically used in binary classification tasks where the \n",
    "goal is to predict a binary label (e.g. positive or negative). \n",
    "\n",
    "It measures the cross-entropy loss between the predicted probabilities and the\n",
    "true labels.\n",
    "\n",
    "The binary_cross_loss instance can be later used to compute the loss between \n",
    "the predicted output and the ground truth during the training of a model.\n",
    "\n",
    "\"\"\"\n",
    "binary_cross_loss = tf.losses.BinaryCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Using the Adam optimizer from the Keras API in TensorFlow. \n",
    "\n",
    "The optimizer is being initialized with a learning rate of 0.0001 (1e-4), \n",
    "which will be used to update the model's weights during training.\n",
    "\n",
    "The Adam optimizer is a popular optimization algorithm used for training deep\n",
    "neural networks.\n",
    "\n",
    "It is an adaptive learning rate optimization algorithm that computes individual\n",
    "adaptive learning rates for different parameters from estimates of first and\n",
    "second moments of the gradients.\n",
    "\n",
    "This helps to update the weights in a more efficient way compared to traditional\n",
    "optimization methods, such as stochastic gradient descent (SGD).\n",
    "\n",
    "In this code, the Adam optimizer is being used with a learning rate of 0.0001,\n",
    "which is a common value for many deep learning tasks. \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(1e-4) # 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Establish Checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up a TensorFlow checkpointing mechanism for saving and restoring the state of a model during training.\n",
    "\n",
    "Once this tf.train.Checkpoint object is created, it can be used to save and restore the state of the optimizer and the model during training.\n",
    "\n",
    "For example, you can call:\n",
    "```python\n",
    "checkpoint.save(file_prefix=checkpoint_prefix) \n",
    "```\n",
    "to save the current state of the model and optimizer to a checkpoint file.\n",
    "\n",
    "Later, you can restore the model and optimizer to the state saved in the checkpoint by calling :\n",
    "    \n",
    "```python\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir)).\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"a directory called \"training_checkpoints\" is created in the current working \n",
    "directory. This is the directory where the checkpoint files will be stored.\"\"\"\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "\n",
    "\"\"\"a variable checkpoint_prefix is defined, which is the prefix that will be \n",
    "used for the checkpoint files. The prefix is defined as the path to the \n",
    "checkpoint directory followed by the string \"ckpt\".\"\"\"\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt')\n",
    "\n",
    "\"\"\"a tf.train.Checkpoint object is created. This object takes two arguments: \n",
    "opt and siamese_model. \n",
    "The opt argument is the optimizer object that will be checkpointed, which in \n",
    "this case is the Adam optimizer defined earlier. \n",
    "The siamese_model argument is the Siamese network model that will be \n",
    "checkpointed.\"\"\"\n",
    "checkpoint = tf.train.Checkpoint(opt=opt, siamese_model=siamese_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Build Train Step Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a training step function for a Siamese neural network using TensorFlow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Perform a single training step for a Siamese neural network.\n",
    "\n",
    "Args:\n",
    "-----\n",
    "    batch: A batch of training data, containing the anchor and positive/negative images\n",
    "           and their corresponding labels.\n",
    "           \n",
    "Returns:\n",
    "-------\n",
    "    The loss value for this batch.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "A decorator that converts the train_step() function into a TensorFlow graph.\n",
    "\n",
    "This can improve performance by allowing the function to be compiled and \n",
    "optimized by TensorFlow's XLA compiler.\n",
    "\"\"\"\n",
    "@tf.function\n",
    "\n",
    "#Define a training step function for a Siamese neural network using TensorFlow. \n",
    "def train_step(batch):\n",
    "    \n",
    "    # Create a gradient tape context, which records all the operations that occur within this block.\n",
    "    with tf.GradientTape() as tape:     \n",
    "       \n",
    "        # Get the first two elements from the batch input, which should be the anchor and positive/negative images.\n",
    "        X = batch[:2]\n",
    "        \n",
    "        # Get the third element from the batch input, which should be the label.\n",
    "        y = batch[2]\n",
    "        \n",
    "        # Perform a forward pass through the Siamese network using the input images X, with the training=True argument indicating that we're in training mode.\n",
    "        yhat = siamese_model(X, training=True)\n",
    "        \n",
    "        # calculates the binary cross-entropy loss between the predicted yhat and the true label y.\n",
    "        loss = binary_cross_loss(y, yhat)\n",
    "    \n",
    "    # This can be useful for debugging or monitoring the training process, but can be removed if desired.\n",
    "    print(loss)\n",
    "        \n",
    "    # Calculate the gradients of the loss with respect to the trainable variables in the Siamese model using the gradient tape.\n",
    "    grad = tape.gradient(loss, siamese_model.trainable_variables)\n",
    "    \n",
    "    # Apply the calculated gradients to the trainable variables using the Adam optimizer.\n",
    "    opt.apply_gradients(zip(grad, siamese_model.trainable_variables))\n",
    "        \n",
    "    # Return the value of the loss for this batch.\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Build Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision and Recall are commonly used to evaluate the performance of classification models.\n",
    "\n",
    "**Precision** measures the proportion of true positives (i.e., correctly classified positive examples) among all examples classified as positive.\n",
    "\n",
    "It is calculated as:\n",
    "```\n",
    "true_positives / (true_positives + false_positives).\n",
    "```\n",
    "**Recall** measures the proportion of true positives among all actual positive examples. \n",
    "\n",
    "It is calculated as:\n",
    "```\n",
    "true_positives / (true_positives + false_negatives).\n",
    "```\n",
    "Both metrics range from 0 to 1, with higher values indicating better performance.\n",
    "\n",
    "They can be used to evaluate binary classification models or multiclass models that have been converted to a set of binary classification problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function takes two inputs: data, which is the training dataset, and EPOCHS, which is the number of epochs to train for.\n",
    "def train(data, EPOCHS):\n",
    "    \n",
    "    # Loop through epochs\n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        \n",
    "        #Prints the current epoch number to the console.\n",
    "        print(f'\\n Epoch {epoch}/{EPOCHS}')\n",
    "        \n",
    "        #Initializes a progress bar object to track the progress of the training.\n",
    "        progbar = tf.keras.utils.Progbar(len(data))\n",
    "        \n",
    "        \"\"\"Initializes the Recall and Precision metrics, which will be used to \n",
    "        track the model's performance during training.\"\"\"\n",
    "        r = Recall()\n",
    "        p = Precision()\n",
    "        \n",
    "        # Loop through each batch\n",
    "        for idx, batch in enumerate(data):\n",
    "            \n",
    "            #Calls the train_step() function to perform a single training step on the current batch.\n",
    "            loss = train_step(batch)\n",
    "            \n",
    "            #Update the Recall and Precision metrics using the true labels batch[2] and the predicted labels yhat.\n",
    "            yhat = siamese_model.predict(batch[:2])\n",
    "            r.update_state(batch[2], yhat)\n",
    "            p.update_state(batch[2], yhat) \n",
    "            \n",
    "            #Updates the progress bar to show the progress of the current epoch.\n",
    "            progbar.update(idx+1)\n",
    "        \n",
    "        #Prints the loss and the current values of the Recall and Precision metrics to the console.\n",
    "        print(loss.numpy(), r.result().numpy(), p.result().numpy())\n",
    "        \n",
    "        \"\"\"saves a checkpoint of the model weights every 10 epochs, \n",
    "        using the (checkpoint_prefix) that was defined earlier.\"\"\"\n",
    "        if epoch % 10 == 0: \n",
    "            checkpoint.save(file_prefix=checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the number of epochs to 50 and calls the train function with the train_data and EPOCHS as arguments to train the Siamese neural network model for 50 epochs.\n",
    "\n",
    "The train function iterates over the training data for each epoch and updates the weights of the model to minimize the loss between the anchor, positive, and negative images. \n",
    "\n",
    "After each epoch, the function prints the average loss and accuracy of the training data. \n",
    "\n",
    "By training for more epochs, the model will have more opportunities to learn the features and patterns of the input images and potentially improve its performance on the task of one-shot image recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "train(train_data, EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 6. Evaluate Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Calculate Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Evaluate the trained siamese model on the test data and calculate precision and recall metrics.\n",
    "\n",
    "Args:\n",
    "-----\n",
    "- test_data: a tensorflow Dataset object containing test data in the form of (input_1, input_2, label) tuples\n",
    "- siamese_model: the trained siamese model to evaluate\n",
    "\n",
    "Returns:\n",
    "-------\n",
    "- The calculated recall and precision values for the model on the test data\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "# Initialize precision and recall metric objects\n",
    "r = Recall()\n",
    "p = Precision()\n",
    "\n",
    "# Loop through each batch in the test data and update the metrics\n",
    "for test_input, test_val, y_true in test_data.as_numpy_iterator():\n",
    "    # Make predictions on the current batch\n",
    "    yhat = siamese_model.predict([test_input, test_val])\n",
    "    \n",
    "    # Update the precision and recall metrics with the true labels and predicted labels\n",
    "    r.update_state(y_true, yhat)\n",
    "    p.update_state(y_true,yhat) \n",
    "\n",
    "# Print the final values of precision and recall\n",
    "print(r.result().numpy(), p.result().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Show the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plots the first image from the `test_input` and `test_val` datasets \n",
    "side-by-side.\n",
    "\"\"\"\n",
    "\n",
    "# Set plot size \n",
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "# Set first subplot\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(test_input[0])\n",
    "\n",
    "# Set second subplot\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(test_val[0])\n",
    "\n",
    "# Renders the plot on the screen\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Save Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Save the model as a regular TensorFlow model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the weights of the siamese_model to a file named \"siamesemodel_v1.h5\".\n",
    "siamese_model.save('siamesemodel_v1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Save the model as a TFLite model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A TFLite model is designed to be more efficient and lightweight compared to a regular TensorFlow model. It is optimized for mobile and edge devices, making it suitable for running models on mobile phones or other devices with limited resources. Running a TFLite model in the cloud can also be more efficient as it reduces the amount of data that needs to be transferred, improving the overall performance of the model.\n",
    "\n",
    "The code provided is likely written to prepare the project for a TFLite model by optimizing it for mobile and edge devices. This may include simplifying the model architecture or reducing the size of the input and output data. \n",
    "\n",
    "Unfortunately, creating a TFLite model can be challenging, especially with limited resources. The Google Colab environment has some limitations in the free phase, including RAM limitations, which may make it difficult to train a model and convert it to TFLite format. However, it is possible to use other cloud services or dedicated hardware to train and convert TFLite models.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(siamese_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "tflite_qaware_model = converter.convert()\n",
    "with open('tflite_model.tflite', 'wb') as f:\n",
    "    f.write(tflite_qaware_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Real Time Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 Load a saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    " Load a previously saved model with custom loss functions and returns the \n",
    " loaded model.\n",
    " \n",
    "Parameters:\n",
    "-----------\n",
    "\n",
    "siamese_model: \n",
    "    A tf.keras.Model object that will be used to load the saved model.\n",
    "    \n",
    "'siamesemodel_v1.h5': \n",
    "    A string representing the name of the file where the model was saved.\n",
    "    \n",
    "custom_objects: \n",
    "    is used to specify additional custom layers or functions that are not \n",
    "    included in the standard Keras modules. When loading a model with custom \n",
    "    layers or loss functions, it is important to provide the corresponding \n",
    "    implementation so that the model can be properly reconstructed.\n",
    "    \n",
    "L1Dist:\n",
    "    A custom distance metric that was used in the construction of the \n",
    "    Siamese network model. The L1Dist function computes the L1 distance between\n",
    "    two vectors. It is not a standard Keras loss function and thus needs to be\n",
    "    passed as a custom object to load_model() function, otherwise the loading \n",
    "    process would fail with an error.\n",
    "    \n",
    "BinaryCrossentropy:\n",
    "    An object defines a custom loss function.\n",
    "\n",
    "Returns:\n",
    "-------\n",
    "\n",
    "A tf.keras.Model object that contains the loaded model with the custom loss\n",
    "functions.\n",
    "\n",
    "\"\"\"\n",
    "siamese_model = tf.keras.models.load_model(\n",
    "    'siamesemodel_v1.h5', custom_objects={\n",
    "        'L1Dist':L1Dist, 'BinaryCrossentropy':tf.losses.BinaryCrossentropy\n",
    "    }\n",
    ")\n",
    "\n",
    "# View model summary\n",
    "siamese_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 Use the model to make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Using the trained Siamese neural network model to make predictions on the test \n",
    "input pairs test_input and test_val.\n",
    "\n",
    "The model then returns a distance measure between the two inputs.\n",
    "\n",
    "The distance measure can be interpreted as a measure of similarity, \n",
    "with smaller distances indicating greater similarity between the inputs.\n",
    "\n",
    "The actual interpretation of the distance measure will depend on the specific\n",
    "implementation of the model.\n",
    "\"\"\"\n",
    "\n",
    "siamese_model.predict([test_input, test_val])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3 Verification Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Verify the given model using a set of verification images.\n",
    "\n",
    "Args:\n",
    "-----\n",
    "    model (keras.Model): \n",
    "        A trained Keras model to be verified.\n",
    "    \n",
    "    detection_threshold (float): \n",
    "        The threshold above which a prediction is considered positive.\n",
    "    \n",
    "    verification_threshold (float): \n",
    "        The proportion of positive predictions required to pass verification.\n",
    "\n",
    "Returns:\n",
    "-------\n",
    "    Tuple: A tuple containing:\n",
    "        - results:\n",
    "            A list of results for each verification image\n",
    "        - verified:\n",
    "            boolean indicating whether the verification threshold has been met.\n",
    "\"\"\"\n",
    "\n",
    "def verify(model, detection_threshold, verification_threshold):\n",
    "    # Build results array\n",
    "    results = []\n",
    "    # Loop through each verification image\n",
    "    for image in os.listdir(os.path.join('application_test', 'verification_images')):\n",
    "        # Preprocess the input image\n",
    "        input_img = preprocess(os.path.join('application_test', 'input_image', 'input_image.jpg'))\n",
    "        # Preprocess the current verification image\n",
    "        validation_img = preprocess(os.path.join('application_test', 'verification_images', image))\n",
    "        \n",
    "        # Make Predictions \n",
    "        result = model.predict(list(np.expand_dims([input_img, validation_img], axis=1)))\n",
    "        # Add the prediction result to the results array\n",
    "        results.append(result)\n",
    "    \n",
    "    # Detection Threshold: Metric above which a prediction is considered positive \n",
    "    detection = np.sum(np.array(results) > detection_threshold)\n",
    "    \n",
    "    # Verification Threshold: Proportion of positive predictions / total positive samples \n",
    "    verification = detection / len(os.listdir(os.path.join('application_test', 'verification_images'))) \n",
    "    # Check if the verification threshold has been met\n",
    "    verified = verification > verification_threshold\n",
    "    \n",
    "    # Return the results and the verification status\n",
    "    return results, verified\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.4  Real Time Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Captures video from the default camera, crops the frame to a fixed size,\n",
    "and displays it on screen.\n",
    "\n",
    "Waits for the 'v' key to be pressed to trigger verification using a pre-trained\n",
    "Siamese neural network model.\n",
    "\n",
    "Prints the verification result to the console and continues to display the\n",
    "camera feed until the 'q' key is pressed.\n",
    "\"\"\"\n",
    "\n",
    "# Set up the video capture object to read from the default camera (index 0)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Loop over the video frames as long as the capture object is open\n",
    "while cap.isOpened():\n",
    "    # Read a frame from the video capture object\n",
    "    ret, frame = cap.read()\n",
    "    # Crop the frame to a fixed size to remove unwanted parts\n",
    "    frame = frame[120:120+250,200:200+250, :]\n",
    "    \n",
    "    # Display the cropped frame on screen using OpenCV's imshow function\n",
    "    cv2.imshow('Verification', frame)\n",
    "    \n",
    "    # Wait for a key event to occur (with a delay of 10 milliseconds)\n",
    "    if cv2.waitKey(10) & 0xFF == ord('v'):\n",
    "        # If the 'v' key is pressed, save the current frame as the input image and run verification\n",
    "        cv2.imwrite(os.path.join('application_data', 'input_image', 'input_image.jpg'), frame)\n",
    "        results, verified = verify(siamese_model, 0.5, 0.5)\n",
    "        # Print the verification result (True or False) to the console\n",
    "        print(verified)\n",
    "    \n",
    "    # If the 'q' key is pressed, break out of the loop and stop the video capture\n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture resources and destroy the OpenCV windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.5 Load a saved TFLite model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load a TFLite model, preprocess input images, and run inference to get predicted values.\n",
    "\n",
    "Args:\n",
    "-----\n",
    "    model_path (str): The path to the TFLite model file.\n",
    "    input_img_path (str): The path to the input image file.\n",
    "    validation_img_path (str): The path to the validation image file.\n",
    "    input_shape (tuple): The shape of the input tensor.\n",
    "    \n",
    "Returns:\n",
    "-------\n",
    "    output_data (np.ndarray): A numpy array of the predicted values.\n",
    "\"\"\"\n",
    "# Load TFLite model\n",
    "interpreter = tf.lite.Interpreter(model_path=\"tflite_model.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensor details\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Define input shape\n",
    "input_shape = (105, 105, 3)\n",
    "\n",
    "# Load and preprocess input images\n",
    "input_img = cv2.imread(\"application_test/9/aaa.jpg\")\n",
    "validation_img = cv2.imread(\"application_test/9/bbb.jpg\")\n",
    "\n",
    "# Resize input and validation images to (105, 105)\n",
    "input_img = cv2.resize(input_img, input_shape[:2])\n",
    "validation_img = cv2.resize(validation_img, input_shape[:2])\n",
    "\n",
    "# Rescale input and validation images to [0, 1]\n",
    "input_img = input_img.astype(np.float32) / 255.0\n",
    "validation_img = validation_img.astype(np.float32) / 255.0\n",
    "\n",
    "# Create input data array with two images\n",
    "input_data = np.array([input_img, validation_img], dtype=np.float32)\n",
    "\n",
    "# Select first image in the input data array\n",
    "input_data = input_data[0]\n",
    "\n",
    "# Add batch dimension to the selected image\n",
    "input_data = np.expand_dims(input_data, axis=0)\n",
    "\n",
    "# Set input tensor data\n",
    "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "# Run inference\n",
    "interpreter.invoke()\n",
    "\n",
    "# Get output tensor data\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "\n",
    "# Print predicted values\n",
    "print(output_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
